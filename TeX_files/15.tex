Если рейтинги вещественные, то можно свести к задаче регресии. Тогда метрики будут MSE, MAE, RMSE и так далее. Если же мы сводим к задаче классификации, также можно взять знакомые метрики вроде F-меры, AUC ROC и т.д. Но рекомендательная система же выдает какое-то ограниченное число айтемов (и нам важно, что находится именно в топе наших рекомендаций). Тогда предполагаем, что рек.система выдает ранжированный список айтемов, и будем считать, что мы показываем пользователю top-k рекомендаций. Поэтому на этом числе k мы и будем считать качество. \\ 

hitrate@k = [$R_{u}(k) \cap L_{u} \ne \varnothing$], где $R_{u}(k)$ – наши top-k рекомендаций, $L_{u}$ -- айтемы, которые пользователю действительно понравились. То есть это индикатор того, что хотя бы 1 из наших рекомендованных айтемов понравился пользователю. \\ 

precision@k = $\dfrac{|R_{u}(k) \cap L_{u}|}{k}$ -- сколько айтемов из тех, что мы порекомендовали, пользователю действительно понравилось. \\ 

recall@k = $\dfrac{|R_{u}(k) \cap L_{u}|}{|L_{u}|}$ -- показыыает, насколько мы смогли вытянуть в топ релевантный контент. \\ 

Есть метрики более специфичные для рекомендаций -- это метрики качества ранжирования. Неплохо было бы учитывать, в каком порядке мы выдаем рекоемндации (айтемы, которые по идее должны сильно зайти пользователю, должны идти первыми, например). \\ 

Пусть $a_{ui} = a(u, i)$, то есть предсказание нашей системы по этому юзеру и по этому айтема. Далее мы сортируем айтемы по возрастанию $a_{ui}$ для конкретного юезра u (и берем первые k рекомендаций). Тогда для этого списка рекомендаций посчитать меру DCG, которая будет накидывать штраф за позицию айтема в ранджировании. \\ 

DCG@k = $\sum\limits_{p = 1}^{k} g(r_{uip})d(p)$ \\ 

p -- номер позиции в ранжировании, $r_{uip}$ -- оценка, которую пользователь u поставил айтему на позиции p (истинные рейтинги). $g(r_{uip}) = 2^{r} - 1$ или $g(r) = r_{uip}$.  $d(p) = \dfrac{1}{\log(p + 1)}$, это как раз штраф за позицию. Например, если у нас взаимодействие может принимать только значения 0 и 1 (например, был клик или нет), то мы просто будем складывать 1/log(1 + p), и эта сумма будет больше, если единички у нас оказываются в начале списка рекомендаций. \\ 

Можно посчитать еще normalized DCG@k(u) = $\frac{DCG@k(u)}{maxDCG@k(u)}$. \\

Как еще можно измерить качество рекомендательных систем? \\ 
1. Novelty (число айтемов, которые раньше пользователю не рекомендовали) \\ 
2. Разнообразие. Можно посчитать расстояние между эмбеддингами айтемов, которые мы порекомендовали. Или использовать метаинформацию об айтемах (жанры, исполнители) и посчитать ее дисперсию. \\ 
3. Serendipity -- умение рекомендовать редкие айтемы, которые пользователю заходят. Например, b -- новая книга, B -- множество книг, которые пользователь уже оценил, $C_{Bw}$ -- число книг автора w в множестве B, $C_{B}$ -- максимальное число книг одного автора в B. Тогда мы можем измерить сходство между b и B. \\ 
d(b, B) = $\frac{1 + C_{B} - C_{Bw}}{1 + C_{B}}$ \\ 
То есть рекомендация будет новее, если юзер еще не видел айтемы этого автора/испольнителя/etc.
