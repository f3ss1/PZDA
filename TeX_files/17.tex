Для каждого пользователя мы имеем d-мерный вектор $p_{u}$, для каждого айтема -- d-мерный вектор $q_{i}$. Мы хотим, чтобы $<p_{u}, q_{i}> \approx r_{ui}$, то есть чтобы скалярное произведение этих векторов аппроксимировало истинный рейтинг. \\ 

В вольной интерпретации (с лекции) можно представить, что d -- это число жанров вообще среди наших айтемов. В $p_{u}$ мы имеем набор оценок, насколько юзеру нравится каждый из жанров, а в $q_{i}$ -- распределение жанров в конкретном айтеме. Тогда $<p_{u}, q_{i}>$ -- мера сонаправленности, чем больше метч между юзером и айтемом, тем больше скалярное произведение. По факту конечно внутри векторов не жанры и предпочтения юзеров, а какие латентные неинтерпретируемые факторы, которые рек.система сама выделила. \\ 

Функционал: $\sum\limits_{(u, i, r_{ui}) \in \mathbb{R}}(r_{ui} - b_{u} - b_{i} - <p_{u}, q_{i}>)^{2} \rightarrow min_{b_{u}, b_{i}, q_{i}, p_{u}}$  \\ 

То есть мы не просто считаем квадратичную ошибку между истинным рейтингом и нашей аппроксимацией скалярным произведением векторов, но и отнимаем из истинного рейтинга среднюю оценку пользователя на всех айтемах ($b_{u}$) и среднюю оценку этого айтема ($b_{i}$) (т.к., например, какие-то юзеры всем ставлят только 4-5 звездочек, и для них 4 это скорее плохо, а кто-то очень сильно придирается, и для них 4 это круто). По сути в этом функционале мы обучаем оба элемента скалярного произведения -- и вектор пользователя, и вектор айтема. Еще к этому функционалу можно накинуть регуляризацию (потребовать, чтобы нормы векторов были не очень большими) ($\lambda\sum||p_{u}||^{2} + \mu\sum||q_{i}||^{2}$).  \\ 

В чем тут есть проблема? Мы тут перемножаем параметры, которые обучаем, это плохо (функция не выпуклая, локальные минимумы появляются и все такое). \\ 

По факту у нас есть матрица P, где лежат все вектора для пользователей, и матрица Q, где лежат все вектора для айтемов, и мы пытаемся осуществить приближение $P^{T}Q \approx R$. \\ 

Как обучаем? \\ 
1) SGD. На каждой итерации берем случайные пары u, i, $r_{ui}$. Но это довольно плохо работает \\ 
2) Alternating Least Squares (ALS). Как-то инициализируем матрицы P и Q. Далее фиксируем Q, обучаем на функционал. Тогда вектора $q_{i}$ оказываются фиксированы, они типа как постоянные признаки, а вектора $p_{u}$ -- это типа как веса при признаках. Получается что-то вроде линейной модели. И так для каждого пользователя мы натрасиваем вектор p. Потом фиксируем матрицу P и подбираем вектора в Q и вот так шаги меняются. \\ 
Это работает лучше, так как при фиксированнии части скаларного изображения получается нормальная линейная выпуклая функция (как в обычных линейных моделях). \\ 

Практический нюанс (хз надо или нет но пусть будет). Потоянно обновлять матрицы вычислительно тяжко. А айтемов у нас обычно сильно меньше, чем пользователей. Тогда мы можем хранить только сравнительно небольшую матрицу Q. При появлении нового юзера подбираем для него $p_{u}$ при фиксированном Q, и делаем рекомендацию через $<p_{u}, q_{i}>$ (делаем один шаг в ALS).  То есть векторы айтемов мы фиксируем, а векторы пользователей постоянно дообучаем на основе последних действий. Раз в сутки/неделю/месяц/хз полностью осуществляем ALS и обновляем матрицу Q тоже. 
